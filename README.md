# ğŸ§  AI Study Assistant

A fully offline, AI-powered study tool that lets you upload PDF documents and ask questions about them. Powered by local LLMs (like LLaMA 3 via Ollama), semantic search (FAISS), and a clean Streamlit interface â€” all with no paid API keys or internet connection required.

> Built with ğŸ’¡ education, ğŸ” privacy, and ğŸƒâ€â™‚ï¸ speed in mind.

---

## âœ¨ Features

- ğŸ“„ Upload and process any PDF (lecture notes, articles, etc.)
- ğŸ§  Ask questions about your documents using a local AI model
- ğŸ—‚ï¸ Embeds content using `sentence-transformers` (MiniLM)
- ğŸ” Finds relevant sections via FAISS vector search
- ğŸ’¬ Generates answers with LLaMA (via [Ollama](https://ollama.com))
- ğŸ–¥ï¸ Streamlit-powered interface
- ğŸš« 100% offline â€” no OpenAI API or internet needed
- ğŸ› ï¸ Modular code structure for easy upgrades

---

## ğŸ”§ Tech Stack

| Layer         | Tools & Libraries                                      |
|---------------|--------------------------------------------------------|
| LLM Backend   | [Ollama](https://ollama.com) + LLaMA 3 / Mistral       |
| Embeddings    | `sentence-transformers` (`all-MiniLM-L6-v2`)           |
| Vector Search | FAISS                                                  |
| PDF Parsing   | PyMuPDF (`fitz`)                                       |
| UI            | Streamlit                                              |
| Language      | Python 3.10+                                           |

---

## ğŸ“ Project Structure
```
AI-Assist/
â”œâ”€â”€ app.py                  # Streamlit frontend
â”œâ”€â”€ main.py                 # Core logic (PDF processing, Q&A)
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ data/                   # Stores FAISS index (auto-created)
â”œâ”€â”€ temp/                   # Uploaded PDFs (auto-created)
â””â”€â”€ modules/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ pdf_parser.py       # Extract text from PDFs
    â”œâ”€â”€ embedder.py         # Generate sentence embeddings
    â”œâ”€â”€ vector_store.py     # FAISS index handling
    â”œâ”€â”€ llm_interface.py    # Communicate with local LLaMA via Ollama
    â””â”€â”€ utils.py            # Helpers (text chunking, etc.)
```

---

## ğŸš€ Getting Started

### 1. Clone the repo

git clone https://github.com/DhanushVellingiri/AI-StudyAssistant.git
cd AI-StudyAssistant
2. Install dependencies
bash
Copy
Edit
pip install -r requirements.txt
3. Install & run Ollama
Download from: https://ollama.com

In a separate terminal:

bash
Copy
Edit
ollama run llama3
(You can also use mistral or other smaller models.)

4. Launch the app
bash
Copy
Edit
streamlit run app.py
Then open http://localhost:8501 in your browser.

ğŸ“¸ Screenshots
ğŸ“¤ Upload a PDF

ğŸ’¬ Ask questions

(Add your own screenshots in a docs/screenshots/ folder and update these)

ğŸ’¡ Future Enhancements
ğŸ“š Multi-document support

ğŸ§  Quiz generation mode

ğŸ—£ï¸ Voice-based Q&A using Whisper

ğŸ“ Save chat history

ğŸŒ Deployable demo version

ğŸ‘¨â€ğŸ’» Author
Dhanush Vellingiri
ğŸ“« LinkedIn
ğŸŒ Portfolio (optional)

ğŸªª License
MIT License â€” feel free to fork, learn, and build on top.

â­ If you like this project...
Give it a â­ on GitHub!

Share your feedback

Connect with me!
